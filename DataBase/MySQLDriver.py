import mysql.connector
from mysql.connector import errorcode
from DatabaseDriver import DatabaseDriver
from typing import List, Dict, Any, Tuple
import threading
import os
import time
from queue import Queue
import json

class MySQLDriver(DatabaseDriver):
    """MySQL æ–¹è¨€å®ç° - å®Œæ•´ TPC-C schema æ”¯æŒ"""

    def connect(self) -> bool:
        try:
            self.connection = mysql.connector.connect(
                host=self.config.get("host", "localhost"),
                port=self.config.get("port", 3306),
                user=self.config["user"],
                password=self.config["password"],
                database=self.config.get("database"),
                allow_local_infile=True
            )
            self.is_connected = True
            return True
        except mysql.connector.Error as err:
            print(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {err}")
            self.is_connected = False
            return False

    def disconnect(self) -> bool:
        if self.connection:
            self.connection.close()
            self.is_connected = False
        return True

    def execute_query(self, query: str) -> List[Dict[str, Any]]:
        if not self.is_connected:
            raise RuntimeError("æ•°æ®åº“æœªè¿æ¥")
        cursor = self.connection.cursor(dictionary=True)
        cursor.execute(query)
        rows = cursor.fetchall()
        cursor.close()
        return rows

    def execute_statement(self, statement: str) -> bool:
        if not self.is_connected:
            raise RuntimeError("æ•°æ®åº“æœªè¿æ¥")
        try:
            cursor = self.connection.cursor()
            cursor.execute(statement)
            self.connection.commit()
            cursor.close()
            return True
        except mysql.connector.Error as err:
            print(f"æ‰§è¡Œè¯­å¥å¤±è´¥: {err}\nSQL: {statement}")
            return False

    def drop_schema(self) -> bool:
        dbname = self.config.get("database", "tpcch")
        return self.execute_statement(f"DROP DATABASE IF EXISTS {dbname}")

    def create_schema(self) -> bool:
        """å®Œæ•´åˆ›å»º TPC-C schema"""
        dbname = self.config.get("database", "tpcch")
        statements = [
            "CREATE DATABASE tpcch",
            """CREATE TABLE tpcch.warehouse (
                w_id integer,
                w_name char(10),
                w_street_1 char(20),
                w_street_2 char(20),
                w_city char(20),
                w_state char(2),
                w_zip char(9),
                w_tax decimal(4,4),
                w_ytd decimal(12,2),
                PRIMARY KEY (w_id)
            )""",
            """CREATE TABLE tpcch.district (
                d_id tinyint,
                d_w_id integer,
                d_name char(10),
                d_street_1 char(20),
                d_street_2 char(20),
                d_city char(20),
                d_state char(2),
                d_zip char(9),
                d_tax decimal(4,4),
                d_ytd decimal(12,2),
                d_next_o_id integer,
                PRIMARY KEY (d_w_id, d_id)
            )""",
            "CREATE INDEX fk_district_warehouse ON tpcch.district (d_w_id ASC)",
            """CREATE TABLE tpcch.customer (
                c_id smallint,
                c_d_id tinyint,
                c_w_id integer,
                c_first char(16),
                c_middle char(2),
                c_last char(16),
                c_street_1 char(20),
                c_street_2 char(20),
                c_city char(20),
                c_state char(2),
                c_zip char(9),
                c_phone char(16),
                c_since DATE,
                c_credit char(2),
                c_credit_lim decimal(12,2),
                c_discount decimal(4,4),
                c_balance decimal(12,2),
                c_ytd_payment decimal(12,2),
                c_payment_cnt smallint,
                c_delivery_cnt smallint,
                c_data text,
                c_n_nationkey integer,
                PRIMARY KEY(c_w_id, c_d_id, c_id)
            )""",
            "CREATE INDEX fk_customer_district ON tpcch.customer (c_w_id ASC, c_d_id ASC)",
            """CREATE TABLE tpcch.history (
                h_c_id smallint,
                h_c_d_id tinyint,
                h_c_w_id integer,
                h_d_id tinyint,
                h_w_id integer,
                h_date date,
                h_amount decimal(6,2),
                h_data char(24)
            )""",
            "CREATE INDEX fk_history_customer ON tpcch.history (h_c_w_id ASC, h_c_d_id ASC, h_c_id ASC)",
            "CREATE INDEX fk_history_district ON tpcch.history (h_w_id ASC, h_d_id ASC)",
            """CREATE TABLE tpcch.neworder (
                no_o_id integer,
                no_d_id tinyint,
                no_w_id integer,
                PRIMARY KEY (no_w_id, no_d_id, no_o_id)
            )""",
            """CREATE TABLE tpcch.order (
                o_id integer,
                o_d_id tinyint,
                o_w_id integer,
                o_c_id smallint,
                o_entry_d date,
                o_carrier_id tinyint,
                o_ol_cnt tinyint,
                o_all_local tinyint,
                PRIMARY KEY (o_w_id, o_d_id, o_id)
            )""",
            "CREATE INDEX fk_order_customer ON tpcch.order (o_w_id ASC, o_d_id ASC, o_c_id ASC)",
            """CREATE TABLE tpcch.orderline (
                ol_o_id integer,
                ol_d_id tinyint,
                ol_w_id integer,
                ol_number tinyint,
                ol_i_id integer,
                ol_supply_w_id integer,
                ol_delivery_d date,
                ol_quantity smallint,
                ol_amount decimal(6,2),
                ol_dist_info char(24),
                PRIMARY KEY (ol_w_id, ol_d_id, ol_o_id, ol_number)
            )""",
            "CREATE INDEX fk_orderline_order ON tpcch.orderline (ol_w_id ASC, ol_d_id ASC, ol_o_id ASC)",
            "CREATE INDEX fk_orderline_stock ON tpcch.orderline (ol_supply_w_id ASC, ol_i_id ASC)",
            """CREATE TABLE tpcch.item (
                i_id integer,
                i_im_id smallint,
                i_name char(24),
                i_price decimal(5,2),
                i_data char(50),
                PRIMARY KEY (i_id)
            )""",
            """CREATE TABLE tpcch.stock (
                s_i_id integer,
                s_w_id integer,
                s_quantity integer,
                s_dist_01 char(24),
                s_dist_02 char(24),
                s_dist_03 char(24),
                s_dist_04 char(24),
                s_dist_05 char(24),
                s_dist_06 char(24),
                s_dist_07 char(24),
                s_dist_08 char(24),
                s_dist_09 char(24),
                s_dist_10 char(24),
                s_ytd integer,
                s_order_cnt integer,
                s_remote_cnt integer,
                s_data char(50),
                s_su_suppkey integer,
                PRIMARY KEY (s_w_id, s_i_id)
            )""",
            "CREATE INDEX fk_stock_warehouse ON tpcch.stock (s_w_id ASC)",
            "CREATE INDEX fk_stock_item ON tpcch.stock (s_i_id ASC)",
            """CREATE TABLE tpcch.nation (
                n_nationkey tinyint NOT NULL,
                n_name char(25) NOT NULL,
                n_regionkey tinyint NOT NULL,
                n_comment char(152) NOT NULL,
                PRIMARY KEY (n_nationkey)
            )""",
            """CREATE TABLE tpcch.supplier (
                su_suppkey smallint NOT NULL,
                su_name char(25) NOT NULL,
                su_address char(40) NOT NULL,
                su_nationkey tinyint NOT NULL,
                su_phone char(15) NOT NULL,
                su_acctbal decimal(12,2) NOT NULL,
                su_comment char(101) NOT NULL,
                PRIMARY KEY (su_suppkey)
            )""",
            """CREATE TABLE tpcch.region (
                r_regionkey tinyint NOT NULL,
                r_name char(55) NOT NULL,
                r_comment char(152) NOT NULL,
                PRIMARY KEY (r_regionkey)
            )"""
        ]
        success = True
        for stmt in statements:
            if not self.execute_statement(stmt):
                success = False
        return success

    def import_csv(self, table_name: str, csv_file: str, delimiter: str = '|') -> bool:
        """ä½¿ç”¨ LOAD DATA INFILE å¯¼å…¥ CSVï¼Œæé«˜å¤§è§„æ¨¡æ•°æ®åŠ è½½é€Ÿåº¦"""
        if not os.path.exists(csv_file):
            print(f"CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
            return False
        try:
            cursor = self.connection.cursor()
            sql = (
                f"LOAD DATA LOCAL INFILE '{csv_file}' "
                f"INTO TABLE `{table_name}` "
                f"FIELDS TERMINATED BY '{delimiter}' "
                f"LINES TERMINATED BY '\n' "
                # f"IGNORE 1 LINES"
            )
            cursor.execute(sql)
            self.connection.commit()
            cursor.close()
            return True
        except Exception as e:
            print(f"LOAD DATA INFILE å¯¼å…¥å¤±è´¥: {e}")
            return False

    def check_data_integrity(self, warehouse_count=1) -> bool:
        """æ£€æŸ¥ TPC-C/H æ•°æ®å®Œæ•´æ€§"""
        tables = {
            "warehouse": 1*warehouse_count,   # å¯æ ¹æ®å®é™… warehouse æ•°é‡è°ƒæ•´
            "district": 10*warehouse_count,   # æ¯ warehouse 10 ä¸ª district
            "customer": 30000*warehouse_count, # æ¯ district 3000 customer
            "order": 30000*warehouse_count,
            "orderline": 300000*warehouse_count,
            "neworder": 9000*warehouse_count,
            "history": 30000*warehouse_count,
            "stock": 100000*warehouse_count,
            "item": 100000*warehouse_count,
            "supplier": 10000*warehouse_count,
            "nation": 62*warehouse_count,
            "region": 5*warehouse_count,
            
        }
        dbname = self.config.get("database", "tpcch")
        for table, expected_count_per_warehouse in tables.items():
            query = f"SELECT COUNT(*) AS cnt FROM `{table}`"
            try:
                rows = self.execute_query(query)
                cnt = rows[0]['cnt']
                print(f"{table} è¡Œæ•°: {cnt}")
                if cnt == 0:
                    print(f"è¡¨ {table} æ•°æ®ä¸å®Œæ•´")
                    return False
            except Exception as e:
                print(f"æ£€æŸ¥ {table} å¤±è´¥: {e}")
                return False
        return True
    

    def _create_thread_connection(self):
        """ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥"""
        return mysql.connector.connect(
            host=self.config.get("host", "localhost"),
            port=self.config.get("port", 3306),
            user=self.config["user"],
            password=self.config["password"],
            database=self.config.get("database"),   # æ•°æ®åº“å
            allow_local_infile=True                  # æ”¯æŒ LOAD DATA LOCAL INFILE
        )

    def execute_with_timing(self, query: str) -> Tuple[List[Dict], float]:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è®°å½•æ—¶é—´"""
        start_time = time.time()
        try:
            result = self.execute_query(query)
            execution_time = time.time() - start_time
            return result, execution_time
        except Exception as e:
            execution_time = time.time() - start_time
            raise e

    def _execute_concurrent_query(self, query: str, duration: float, results_queue: Queue):
        """å¹¶å‘æ‰§è¡ŒæŸ¥è¯¢çš„çº¿ç¨‹å‡½æ•°"""
        query_count = 0
        thread_times = []
        end_time = time.time() + duration
        
        thread_conn = self._create_thread_connection()
        
        while time.time() < end_time:
            start_time = time.time()
            try:
                if thread_conn:
                    cursor = thread_conn.cursor()
                    cursor.execute(query)
                    cursor.fetchall()
                    cursor.close()
                execution_time = time.time() - start_time
                query_count += 1
                thread_times.append(execution_time)
            except Exception:
                #è®°å½•å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ
                continue
        
        if thread_conn:
            thread_conn.close()
        
        results_queue.put({
            'query_count': query_count,
            'thread_times': thread_times
        })

    def _calculate_throughput_metrics(self, total_queries: int, total_duration: float, 
                                    all_times: List[float]) -> Dict[str, Any]:
        """è®¡ç®—ååé‡ç›¸å…³æŒ‡æ ‡"""
        tps = total_queries / total_duration if total_duration > 0 else 0
        qps = tps  # å¯¹äºæŸ¥è¯¢æ¥è¯´ï¼ŒTPSå’ŒQPSç›¸åŒ
        
        avg_latency = sum(all_times) / len(all_times) if all_times else 0
        min_latency = min(all_times) if all_times else 0
        max_latency = max(all_times) if all_times else 0
        
        throughput_efficiency = (tps / avg_latency) if avg_latency > 0 else 0
        
        return {
            'throughput_tps': tps,
            'throughput_qps': qps,
            'total_queries': total_queries,
            'total_duration': total_duration,
            'avg_latency': avg_latency,
            'min_latency': min_latency,
            'max_latency': max_latency,
            'throughput_efficiency': throughput_efficiency,
            'queries_per_second_per_thread': tps / len(all_times) if all_times else 0
        }

    def _run_concurrent_test(self, query: str, concurrency: int, duration: float) -> Dict[str, Any]:
        """è¿è¡Œå¹¶å‘æµ‹è¯• æµ‹ååé‡"""
        print(f"å¹¶å‘ååé‡æµ‹è¯• - çº¿ç¨‹æ•°: {concurrency}, æŒç»­æ—¶é—´: {duration}ç§’")
        
        results_queue = Queue()
        threads = []
        all_times = []
        total_queries = 0
        
        # è®°å½•æµ‹è¯•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å¯åŠ¨æ‰€æœ‰å¹¶å‘çº¿ç¨‹
        for i in range(concurrency):
            thread = threading.Thread(
                target=self._execute_concurrent_query,
                args=(query, duration, results_queue)
            )
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        # è®¡ç®—æ€»æµ‹è¯•æ—¶é—´
        total_duration = time.time() - start_time
        
        # æ”¶é›†æ‰€æœ‰çº¿ç¨‹çš„ç»“æœ
        while not results_queue.empty():
            result = results_queue.get()
            total_queries += result['query_count']
            all_times.extend(result['thread_times'])
        
        # è®¡ç®—ååé‡æŒ‡æ ‡
        throughput_metrics = self._calculate_throughput_metrics(total_queries, total_duration, all_times)
        
        # æ„å»ºå®Œæ•´ç»“æœ
        results = {
            'test_mode': 'concurrent_throughput',
            'concurrency_level': concurrency,
            'target_duration': duration,
            'actual_duration': total_duration,
        }
        results.update(throughput_metrics)
        
        print(f"å¹¶å‘ååé‡æµ‹è¯•å®Œæˆ - TPS: {throughput_metrics['throughput_tps']:.2f}, "
            f"æ€»æŸ¥è¯¢æ•°: {total_queries}")
        
        return results

    def _run_sequential_test(self, query: str, iterations: int) -> Dict[str, Any]:
        """è¿è¡Œä¸²è¡Œæµ‹è¯• æµ‹å»¶è¿Ÿ"""
        print(f"ä¸²è¡Œå»¶è¿Ÿæµ‹è¯• - è¿­ä»£æ¬¡æ•°: {iterations}")
        
        latencies = []
        successful_runs = 0
        
        start_time = time.time()
        
        for i in range(iterations):
            try:
                _, execution_time = self.execute_with_timing(query)
                latencies.append(execution_time)
                successful_runs += 1
                print(f"æ‰§è¡Œ {i+1}/{iterations}: {execution_time:.4f}s")
            except Exception as e:
                print(f"æ‰§è¡Œ {i+1}/{iterations}: å¤±è´¥ - {e}")
        
        
        total_time = time.time() - start_time
        
        if successful_runs == 0:
            raise RuntimeError("æ‰€æœ‰SQLæ‰§è¡Œéƒ½å¤±è´¥äº†")
        
        #è®¡ç®—ä¸²è¡Œæ¨¡å¼çš„ååé‡
        tps = successful_runs / total_time if total_time > 0 else 0
        avg_latency = sum(latencies) / len(latencies)
        
        results = {
            'test_mode': 'sequential_latency',
            'total_iterations': iterations,
            'successful_iterations': successful_runs,
            'total_time': total_time,
            'throughput_tps': tps,
            'throughput_qps': tps,
            'avg_latency': avg_latency,
            'min_latency': min(latencies),
            'max_latency': max(latencies),
            'latencies': latencies,
            'throughput_efficiency': (tps / avg_latency) if avg_latency > 0 else 0
        }
        
        print(f"ä¸²è¡Œæµ‹è¯•å®Œæˆ - å¹³å‡å»¶è¿Ÿ: {avg_latency:.4f}s, TPS: {tps:.2f}")
        
        return results

    def _warmup_sql(self, sql: str, warmup_runs: int = 3):
        """é¢„çƒ­æ‰§è¡ŒSQL"""
        print("å¼€å§‹é¢„çƒ­...")
        for i in range(warmup_runs):
            try:
                start_time = time.time()
                self.execute_query(sql)
                execution_time = time.time() - start_time
                print(f"é¢„çƒ­æ‰§è¡Œ {i+1}/{warmup_runs}: {execution_time:.4f}s")
            except Exception as e:
                print(f"é¢„çƒ­æ‰§è¡Œ {i+1}/{warmup_runs}: å¤±è´¥ - {e}")

    def evaluation(self, data: str, physical_schema: str, benchmark_sql: str, 
                iterations: int = 10, concurrency: int = 1, 
                duration: float = None) -> Dict[str, Any]:
        """
        L0-evaluationï¼šæ‰§è¡ŒåŸºå‡†SQLå¹¶è¿”å›TPSå’Œlatency
        
        Args:
            data: æ•°æ®
            physical_schema: schemaä¿¡æ¯
            benchmark_sql: æµ‹è¯•SQL
            iterations: æ‰§è¡Œè¿­ä»£æ¬¡æ•°(ä¸²è¡Œ)
            concurrency: å¹¶å‘çº¿ç¨‹æ•°
            duration: å¹¶å‘æµ‹è¯•æŒç»­æ—¶é—´s
            
        Returns:
            åŒ…å«tps latencyç­‰æŒ‡æ ‡çš„dict
        """
        print(f"å¼€å§‹æ€§èƒ½è¯„ä¼°")
        print(f"æ•°æ®: {data}")
        print(f"Schema: {physical_schema}")
        print(f"æµ‹è¯•SQL: {benchmark_sql}")
        
        #é¢„çƒ­
        self._warmup_sql(benchmark_sql)
        
        if concurrency > 1 and duration:
            #å¹¶å‘ååé‡æµ‹è¯•
            results = self._run_concurrent_test(benchmark_sql, concurrency, duration)
        else:
            #ä¸²è¡Œå»¶è¿Ÿæµ‹è¯•
            results = self._run_sequential_test(benchmark_sql, iterations)
        
        # æ·»åŠ å…¬å…±ä¿¡æ¯
        results.update({
            'data': data,
            'physical_schema': physical_schema,
            'benchmark_sql': benchmark_sql,
        })
        
        # è¾“å‡ºä¸»è¦ç»“æœ
        tps_value = results.get('throughput_tps', 0)
        latency_value = results.get('avg_latency', 0)
        print(f"è¯„ä¼°å®Œæˆ - TPS: {tps_value:.2f}, avg_latency: {latency_value:.4f}s")
        
        return results





# def clean_and_save_sql(input_file, output_dir="cleaned_sql"):
#     """æ™ºèƒ½æ¸…æ´—SQLå¹¶ä¿å­˜åˆ°æ–°æ–‡ä»¶"""
    
#     # åˆ›å»ºè¾“å‡ºç›®å½•
#     if not os.path.exists(output_dir):
#         os.makedirs(output_dir)
#         print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
#     # 1. è¯»å–åŸå§‹æ–‡ä»¶
#     print(f"è¯»å–æ–‡ä»¶: {input_file}")
#     with open(input_file, 'r', encoding='utf-8') as f:
#         content = f.read()
    
#     # 2. æ™ºèƒ½æ¸…æ´—SQL
#     sql_blocks = []
#     current_block = []
#     in_sql_block = False
#     query_number = 1
    
#     lines = content.split('\n')
    
#     for line in lines:
#         line = line.strip()
        
#         # æ£€æµ‹æŸ¥è¯¢å¼€å§‹ï¼ˆTPC-H Queryï¼‰
#         if line.startswith('// TPC-H-Query'):
#             if current_block and in_sql_block:
#                 # å®Œæˆå‰ä¸€ä¸ªSQLå—
#                 sql = ''.join(current_block)
#                 sql = sql.replace('\\n', '\n').replace('\\t', '\t').strip()
                
#                 # ç§»é™¤æœ«å°¾é€—å·å¹¶æ·»åŠ åˆ†å·
#                 if sql.endswith(','):
#                     sql = sql[:-1].strip()
#                 if sql and not sql.endswith(';'):
#                     sql += ';'
                
#                 sql_blocks.append((query_number, sql))
#                 query_number += 1
#                 current_block = []
#             in_sql_block = True
#             continue
        
#         # å¤„ç†SQLå­—ç¬¦ä¸²è¡Œ
#         if in_sql_block and line.startswith('"'):
#             sql_line = line.strip('",')
#             sql_line = sql_line.replace('\\n', '\n').replace('\\t', '\t')
#             current_block.append(sql_line)
    
#     # å¤„ç†æœ€åä¸€ä¸ªSQLå—
#     if current_block and in_sql_block:
#         sql = ''.join(current_block)
#         sql = sql.replace('\\n', '\n').replace('\\t', '\t').strip()
#         if sql.endswith(','):
#             sql = sql[:-1].strip()
#         if sql and not sql.endswith(';'):
#             sql += ';'
#         sql_blocks.append((query_number, sql))
    
#     print(f"æˆåŠŸæ¸…æ´—å‡º {len(sql_blocks)} ä¸ªSQLæŸ¥è¯¢")
    
#     # 3. ä¿å­˜ä¸ºä¸åŒæ ¼å¼çš„æ–‡ä»¶
    
    
    
#     # # 3.2 ä¿å­˜ä¸ºå¯æ‰§è¡Œçš„SQLæ–‡ä»¶
#     # sql_file = os.path.join(output_dir, "benchmark_queries.sql")
#     # with open(sql_file, 'w', encoding='utf-8') as f:
#     #     f.write('-- ============================================\n')
#     #     f.write('-- TPC-H Benchmark Queries\n')
#     #     f.write('-- Automatically cleaned and formatted\n')
#     #     f.write('-- Generated from: ' + os.path.basename(input_file) + '\n')
#     #     f.write('-- ============================================\n\n')
        
#     #     for num, sql in sql_blocks:
#     #         f.write(f'-- Query {num}\n')
#     #         f.write('-- ' + '=' * 50 + '\n')
#     #         f.write(sql)
#     #         f.write('\n\n')
    
#     # print(f"âœ“ ä¿å­˜ä¸ºSQLæ–‡ä»¶: {sql_file}")
    
#     # 3.3 ä¿å­˜ä¸ºç‹¬ç«‹çš„SQLæ–‡ä»¶ï¼ˆæ¯ä¸ªæŸ¥è¯¢ä¸€ä¸ªæ–‡ä»¶ï¼‰
#     individual_dir = os.path.join(output_dir, "individual_queries")
#     if not os.path.exists(individual_dir):
#         os.makedirs(individual_dir)
    
#     for num, sql in sql_blocks:
#         query_file = os.path.join(individual_dir, f"query_{num:02d}.sql")
#         with open(query_file, 'w', encoding='utf-8') as f:
#             f.write(f'-- TPC-H Query {num}\n')
#             f.write('-- ' + '=' * 40 + '\n')
#             f.write(sql)
#             f.write('\n')
    
#     print(f"âœ“ ä¿å­˜ä¸ºç‹¬ç«‹æŸ¥è¯¢æ–‡ä»¶: {individual_dir}/")
    
    # # 3.4 ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆå¯é€‰ï¼‰
    # import json
    # json_file = os.path.join(output_dir, "benchmark_queries.json")
    # queries_dict = {f"query_{num}": sql for num, sql in sql_blocks}
    # with open(json_file, 'w', encoding='utf-8') as f:
    #     json.dump(queries_dict, f, indent=2, ensure_ascii=False)
    
    # print(f"âœ“ ä¿å­˜ä¸ºJSONæ–‡ä»¶: {json_file}")
    
    
   
    
    # return sql_blocks
import glob

def load_sql_from_directory(sql_dir="cleaned_sql/individual_queries"):

    """ä»ç›®å½•åŠ è½½æ‰€æœ‰SQLæ–‡ä»¶"""
    sql_files = glob.glob(os.path.join(sql_dir, "*.sql"))
    sql_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
    
    benchmark_sql_list = []
    
    for sql_file in sql_files:
        try:
            with open(sql_file, 'r', encoding='utf-8') as f:
                sql_content = f.read().strip()
            
            # è·å–æŸ¥è¯¢ç¼–å·
            filename = os.path.basename(sql_file)
            query_num = filename.replace('query_', '').replace('.sql', '')
            
            benchmark_sql_list.append({
                'file': sql_file,
                'number': query_num,
                'sql': sql_content
            })
            print(f"âœ“ åŠ è½½æŸ¥è¯¢ {query_num}: {filename}")
            
        except Exception as e:
            print(f"âœ— åŠ è½½å¤±è´¥ {sql_file}: {e}")
    
    print(f"æ€»å…±åŠ è½½ {len(benchmark_sql_list)} ä¸ªSQLæŸ¥è¯¢")
    return benchmark_sql_list



def main():
    # ====== 1. åˆ›å»ºæ•°æ®åº“å¯¹è±¡ ======
    db = MySQLDriver(config={
        "host": "localhost",
        "port": 3306,
        "user": "root",
        "password": "947722",
        "database": "tpcch"
    })

    # ====== 2. è¿æ¥æ•°æ®åº“ ======
    print("=== è¿æ¥æ•°æ®åº“ ===")
    if not db.connect():
        print("æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return

    # ====== 3. åˆ é™¤æ—§ schema ======
    print("=== åˆ é™¤æ—§æ•°æ®åº“ ===")
    db.drop_schema()

    # ====== 4. åˆ›å»ºæ–° schema ======
    print("=== åˆ›å»ºæ–°æ•°æ®åº“ ===")
    if not db.create_schema():
        print("åˆ›å»ºæ•°æ®åº“å¤±è´¥")
        db.disconnect()
        return

    # åˆ‡æ¢åˆ°æ–°åº“ï¼ˆå› ä¸º drop+create åè¿æ¥ä»ä½¿ç”¨æ—§åº“ï¼‰
    db.connection.database = db.config["database"]

    # # ====== 5. åˆ›å»ºå…¨éƒ¨ TPC-C è¡¨ ======
    # print("=== åˆ›å»º TPC-C è¡¨ç»“æ„ ===")
    # schema_sql_path = "./tpcc_schema.sql"    # ä½ è‡ªå·±çš„ schema æ–‡ä»¶
    # if not os.path.exists(schema_sql_path):
    #     print(f"å»ºè¡¨ SQL æ–‡ä»¶ä¸å­˜åœ¨ï¼š{schema_sql_path}")
    #     db.disconnect()
    #     return
    
    # with open(schema_sql_path, "r", encoding="utf-8") as f:
    #     schema_sql = f.read()

    # for statement in schema_sql.split(";"):
    #     stmt = statement.strip()
    #     if stmt:
    #         if not db.execute_statement(stmt + ";"):
    #             print(f"åˆ›å»ºè¡¨å¤±è´¥: {stmt}")
    #             db.disconnect()
    #             return
    tasks = {
        "warehouse": "D:/LLM4DBTuning/tpcc_data/WAREHOUSE.tbl",
        "district": "D:/LLM4DBTuning/tpcc_data/DISTRICT.tbl",
        "customer": "D:/LLM4DBTuning/tpcc_data/CUSTOMER.tbl",
        "history": "D:/LLM4DBTuning/tpcc_data/HISTORY.tbl",
        "order": "D:/LLM4DBTuning/tpcc_data/ORDER.tbl",
        "orderline": "D:/LLM4DBTuning/tpcc_data/ORDERLINE.tbl",
        "item": "D:/LLM4DBTuning/tpcc_data/ITEM.tbl",
        "stock": "D:/LLM4DBTuning/tpcc_data/STOCK.tbl",
        "nation": "D:/LLM4DBTuning/tpcc_data/NATION.tbl",
        "supplier": "D:/LLM4DBTuning/tpcc_data/SUPPLIER.tbl",
        "region": "D:/LLM4DBTuning/tpcc_data/REGION.tbl",
        "neworder": "D:/LLM4DBTuning/tpcc_data/NEWORDER.tbl"
    }


    print("=== å¼€å§‹æ‰¹é‡å¯¼å…¥æ•°æ® ===")
    import_success = False
    for table, tbl_file in tasks.items():
        print(f"\n>>> å¼€å§‹å¯¼å…¥è¡¨ {table} å¯¹åº”æ–‡ä»¶: {tbl_file}")

        if not os.path.exists(tbl_file):
            print(f"[è·³è¿‡] æ–‡ä»¶ä¸å­˜åœ¨: {tbl_file}")
            continue

        result = db.import_csv(table, tbl_file, delimiter="|")

        if result:
            import_success = True
            print(f"[æˆåŠŸ] {table} å¯¼å…¥å®Œæˆ")
        else:
            print(f"[å¤±è´¥] {table} å¯¼å…¥å¤±è´¥")

    if import_success:
            # 3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            integrity_ok = db.check_data_integrity(warehouse_count=1)
            
            if integrity_ok:
                print("\nğŸ‰ TPC-C æ•°æ®å‡†å¤‡å®Œæˆï¼æ‰€æœ‰æ£€æŸ¥é€šè¿‡")
            else:
                print("\nâš ï¸  TPC-C æ•°æ®å‡†å¤‡å®Œæˆï¼Œä½†éƒ¨åˆ†æ•°æ®å¯èƒ½ä¸å®Œæ•´")
        

    print("\n=== å…¨éƒ¨å¯¼å…¥ä»»åŠ¡æ‰§è¡Œå®Œæ¯• ===")

    print("\n=== å¼€å§‹è¿›è¡Œæµ‹è¯•æ€§èƒ½ä»»åŠ¡ ===")
    print("\n=== å¯¼å…¥ch-benchmark 22æ¡SQL ===")

    # æ¸…æ´—sql
    # cleaned_sql_list = clean_and_save_sql("D:\LLM4DBTUNING\sql_queries\sql_queries.txt")

    benchmark_sql_list = load_sql_from_directory()
    print("\n=== å¯¼å…¥SQLæˆåŠŸ ===")

    iterations = 10  # ä¸²è¡Œæ‰§è¡Œæ¬¡æ•°
    max_threads = os.cpu_count()                       # æœ€å¤§çº¿ç¨‹æ•°
    duration = 10      
    # å­˜æ”¾æ‰€æœ‰ SQL æµ‹è¯•ç»“æœ
    all_results = []

    for query in benchmark_sql_list:
        sql_content = query['sql']  # æå–SQLå­—ç¬¦ä¸²
        print(f"æ­£åœ¨æµ‹è¯• Query {query['number']}: {sql_content[:50]}...")
        # ä¸²è¡Œæµ‹è¯•
        seq_result = db.evaluation(
            data="tpcch_data",
            physical_schema="tpcch",
            benchmark_sql=sql_content,
            iterations=iterations,
            concurrency=1  # ä¸²è¡Œ
        )
        
        # å¹¶å‘æµ‹è¯•
        conc_result = db.evaluation(
            data="tpcch_data",
            physical_schema="tpcch",
            benchmark_sql=sql_content,
            concurrency=max_threads,
            duration=duration
        )
        
        all_results.append({
            "sql": sql_content,
            "sequential": seq_result,
            "concurrent": conc_result
        })

    # ä¿å­˜ç»“æœ
    output_file = "tpcc_benchmark_results.json"

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=4)

    print(f"æ‰€æœ‰ SQL æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° {output_file}")



if __name__ == "__main__":
    main()


# python MySQLDriver.py